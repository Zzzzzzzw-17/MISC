{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Coding Tasks "
      ],
      "metadata": {
        "id": "mAKFjHrwzXp1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "r9Fn6QMmzWrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup connection to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "%cd \"/content/drive/My Drive/code-archive\"\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_-0CNPVhS5M",
        "outputId": "1eba14d5-33c7-42aa-e6f6-945645538224"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "/content/drive/My Drive/code-archive\n",
            "'“analysis corpur embeddings.ipynb”的副本'   postag.py\n",
            " assesment_paper.txt\t\t\t    'sentence classification'\n",
            " cyberpunk.ipynb\t\t\t     sst2.py\n",
            " dataset\t\t\t\t     “twitter_analysis.ipynb”的副本\n",
            "'document classification.ipynb'\t\t     wandb\n",
            " logs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "78hpxqCo4V2G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# loading file"
      ],
      "metadata": {
        "id": "9me1-DKC4V4J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open (\"assesment_paper.txt\") as f:\n",
        "  data = f.read()"
      ],
      "metadata": {
        "id": "N6kZOMNvhWG0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task1: Warm-up "
      ],
      "metadata": {
        "id": "WDw3dvFt4a9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = [token for token in data.split(\" \") if token!=\" \"]\n",
        "token_start_clinic = [token for token in tokens if token.startswith(\"clinic\")]\n",
        "print(f\"Count of words that start with “clinic”: {len(token_start_clinic)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtKY6ig2iBT0",
        "outputId": "51b44880-c7ec-47ee-ee8b-5ca873c1a31b"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count of words that start with “clinic”: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_digit = [token for token in tokens if token.isdigit()]\n",
        "print(f\"Count of numbers in the text: {len(token_digit)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-LCzje5ik4A",
        "outputId": "4fa4f84a-77d5-4d0a-b1c4-a060803876da"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count of numbers in the text: 140\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_follow_t = [token for token in tokens if len(token)>1 and token[-1]=='T' and token[:-1].isdigit()]\n",
        "print(f\"Count of numbers followed by “T” (standing for Tesla, the unit of measure): {len(token_follow_t)}\")\n",
        "# An inspection of the corpus indicates there might be space between therefore:\n",
        "token_follow_t_space=[' '.join([tokens[i-1], token]) for i, token in enumerate(tokens) if token=='T' and tokens[i-1].isdigit()]\n",
        "print(token_follow_t_space)\n",
        "token_follow_t_all = token_follow_t + token_follow_t_space\n",
        "print(f\"Count of numbers followed by “T” (standing for Tesla, the unit of measure): {len(token_follow_t_all)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCtZ8lUkjC0d",
        "outputId": "ee78c0fc-319a-436c-9c63-2224f9631d54"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count of numbers followed by “T” (standing for Tesla, the unit of measure): 1\n",
            "['3 T', '3 T', '3 T', '3 T', '3 T', '3 T', '3 T', '3 T']\n",
            "Count of numbers followed by “T” (standing for Tesla, the unit of measure): 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "most_common = Counter(tokens).most_common(1)\n",
        "print(f\"The most common word is {most_common[0][0]}, with {most_common[0][1]} occurrences.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q620QjEnkZeK",
        "outputId": "9a118e10-61f0-41a6-badb-6846f26d0197"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The most common word is , with 770 occurrences.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Task1 results (incase the console does not show anything :) )**\n",
        "\n",
        "Count of words that start with “clinic”: 10\n",
        "\n",
        "Count of numbers in the text: 140\n",
        "\n",
        "Count of numbers followed by “T” (standing for Tesla, the unit of measure): 9\n",
        "\n",
        "The most common word is , with 770 occurrences.\n",
        "\n"
      ],
      "metadata": {
        "id": "I5wJCWLR5Ken"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task2: Understanding the grammar "
      ],
      "metadata": {
        "id": "euD-Y3x74ejP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "clinic_noun = [item for item in nltk.pos_tag(token_start_clinic) if item[1]=='NN' or item[1]=='NNS']\n",
        "print(f\"Count of words that start with “clinic” and that are a noun/noun phrase {len(clinic_noun)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUNpvki9lCDo",
        "outputId": "9321746f-1557-4d4c-84dd-533f37f185af"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count of words that start with “clinic” and that are a noun/noun phrase 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from nltk.corpus.reader.tagged import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "  \n",
        "\n",
        "s1 = \"Another major challenge of lung imaging is the respiratory motion that induces artifacts.\"\n",
        "s2 = \"While breath-hold acquisitions are commonly performed at end-inspiration, free-breathing  acquisitions are usually performed at tidal volumes.\"\n",
        "\n",
        "def common_form(s):\n",
        "    lemmatized = [lemmatizer.lemmatize(item) for item in nltk.word_tokenize(s)]\n",
        "    lemmatized = ' '.join(lemmatized)\n",
        "    print(f\"original sentence: {s}\")\n",
        "    print(f\"lemmatized sentence: {lemmatized}\")\n",
        "    return\n",
        "\n",
        "def get_sub_obj(s):\n",
        "    sub = [tok for tok in nlp(s) if \"subj\" in tok.dep_ ]\n",
        "    obj = [tok for tok in nlp(s) if \"obj\" in tok.dep_ ] \n",
        "    print(f\"subjects: {sub}, objects: {obj}\")\n",
        "    return\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4P_F8uhYlRS8",
        "outputId": "f9bd5c58-4ead-428a-dff1-286ef03328f1"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "common_form(s1)\n",
        "common_form(s2)\n",
        "get_sub_obj(s1)\n",
        "get_sub_obj(s2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWQRj9gAoO6u",
        "outputId": "b352059d-e9b3-43fe-9a82-4f671f75f90d"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original sentence: Another major challenge of lung imaging is the respiratory motion that induces artifacts.\n",
            "lemmatized sentence: Another major challenge of lung imaging is the respiratory motion that induces artifact .\n",
            "original sentence: While breath-hold acquisitions are commonly performed at end-inspiration, free-breathing  acquisitions are usually performed at tidal volumes.\n",
            "lemmatized sentence: While breath-hold acquisition are commonly performed at end-inspiration , free-breathing acquisition are usually performed at tidal volume .\n",
            "subjects: [challenge, that], objects: [imaging, artifacts]\n",
            "subjects: [acquisitions, acquisitions], objects: [inspiration, volumes]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Task2 results**\n",
        "\n",
        "Count of words that start with “clinic” and that are a noun/noun phrase 3\n",
        "\n",
        "lemmatized s1: Another major challenge of lung imaging is the respiratory motion that induces artifact .\n",
        "\n",
        "lemmatized s2: While breath-hold acquisition are commonly performed at end-inspiration , free-breathing acquisition are usually performed at tidal volume .\n",
        "\n",
        "s1: subjects: [challenge, that], objects: [imaging, artifacts]\n",
        "\n",
        "s2: subjects: [acquisitions, acquisitions], objects: [inspiration, volumes]"
      ],
      "metadata": {
        "id": "gXmYss5j5lCn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task3: Identifying entities"
      ],
      "metadata": {
        "id": "IA6vZS9N4ymW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        "entity_sent = \"The UK tradition of eating fish battered and fried in oil was introduced to the country by Spanish and  Portuguese Jewish immigrants, who spent time in the Netherlands before settling in the UK as early as the  16th century.\"\n",
        "doc = nlp(entity_sent)\n",
        "#displacy.serve(doc, style=\"ent\")\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAI9g1xrogL0",
        "outputId": "c73dd29b-ebe2-46d3-d8cb-5173af37826a"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "UK GPE\n",
            "Spanish NORP\n",
            "Portuguese Jewish NORP\n",
            "Netherlands GPE\n",
            "UK GPE\n",
            "the  16th century DATE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loc = [(ent.text,ent.label_) for ent in nlp(\" \".join(tokens)).ents]\n",
        "all_entity_type = [item[1] for item in loc] \n",
        "count = Counter(all_entity_type)\n",
        "print(count)\n",
        "for k,v in count.items():\n",
        "    print(f\"The paper contains {v} times of {k}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaaIxFk0mOi_",
        "outputId": "fb5c4473-8058-4d1c-b1aa-4dd13e302255"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({'CARDINAL': 300, 'ORG': 292, 'PERSON': 194, 'GPE': 80, 'PERCENT': 31, 'QUANTITY': 23, 'DATE': 21, 'PRODUCT': 15, 'NORP': 15, 'ORDINAL': 7, 'MONEY': 5, 'TIME': 2, 'FAC': 2, 'LOC': 1, 'LAW': 1, 'WORK_OF_ART': 1})\n",
            "The paper contains 292 times of ORG\n",
            "The paper contains 194 times of PERSON\n",
            "The paper contains 80 times of GPE\n",
            "The paper contains 300 times of CARDINAL\n",
            "The paper contains 15 times of PRODUCT\n",
            "The paper contains 31 times of PERCENT\n",
            "The paper contains 23 times of QUANTITY\n",
            "The paper contains 21 times of DATE\n",
            "The paper contains 15 times of NORP\n",
            "The paper contains 2 times of TIME\n",
            "The paper contains 1 times of LOC\n",
            "The paper contains 7 times of ORDINAL\n",
            "The paper contains 5 times of MONEY\n",
            "The paper contains 2 times of FAC\n",
            "The paper contains 1 times of LAW\n",
            "The paper contains 1 times of WORK_OF_ART\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python -m spacy download en_core_web_lg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSQpH2oItOrn",
        "outputId": "4d8f6d1f-dfc1-464c-fac6-ddc8b502d432"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "2022-12-13 08:58:35.282779: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-lg==3.4.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.4.1/en_core_web_lg-3.4.1-py3-none-any.whl (587.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 587.7 MB 13 kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from en-core-web-lg==3.4.1) (3.4.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.0.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.0.8)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.23.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (4.64.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.0.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (57.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (21.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.21.6)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.3.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.10.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.11.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.0.9)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.0.7)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.7.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.4.5)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.10.0)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (8.1.5)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (5.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (4.4.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.0.1)\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.4.1\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import en_core_web_lg\n",
        "nlp = en_core_web_lg.load()\n",
        "target = nlp(\"study\")\n",
        "\n",
        "most_similar = {token: target.similarity(nlp(token)) for token in tokens}\n",
        "ordered = {k: v for k, v in sorted(most_similar.items(), key=lambda item: item[1], reverse=True)}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_wP8hCusW1G",
        "outputId": "be55837d-327c-42b4-9a53-94f63a6a1a8c"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-53-4fa4113cbf86>:5: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
            "  most_similar = {token: target.similarity(nlp(token)) for token in tokens}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keys = list(ordered.keys())\n",
        "lemma = [word for word in keys if lemmatizer.lemmatize(word)!=lemmatizer.lemmatize(\"study\") and word[:5]!='study']\n",
        "print(f\"Most similar words to “study” are (descending order): {lemma[0]}, {lemma[1]}, {lemma[2]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tblaLF2Wufed",
        "outputId": "797fae0f-ea93-478c-8f29-75cc49aa7afa"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar words to “study” are (descending order): research, analysis, analyses\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Task3 results**\n",
        "\n",
        "UK GPE\n",
        "\n",
        "Spanish NORP\n",
        "\n",
        "Portuguese Jewish NORP\n",
        "\n",
        "Netherlands GPE\n",
        "\n",
        "UK GPE\n",
        "\n",
        "the  16th century DATE\n",
        "\n",
        "========================================================\n",
        "The paper contains 292 times of ORG\n",
        "\n",
        "The paper contains 194 times of PERSON\n",
        "\n",
        "The paper contains 80 times of GPE\n",
        "\n",
        "The paper contains 300 times of CARDINAL\n",
        "\n",
        "The paper contains 15 times of PRODUCT\n",
        "\n",
        "The paper contains 31 times of PERCENT\n",
        "\n",
        "The paper contains 23 times of QUANTITY\n",
        "\n",
        "The paper contains 21 times of DATE\n",
        "\n",
        "The paper contains 15 times of NORP\n",
        "\n",
        "The paper contains 2 times of TIME\n",
        "\n",
        "The paper contains 1 times of LOC\n",
        "\n",
        "The paper contains 7 times of ORDINAL\n",
        "\n",
        "The paper contains 5 times of MONEY\n",
        "\n",
        "The paper contains 2 times of FAC\n",
        "\n",
        "The paper contains 1 times of LAW\n",
        "\n",
        "The paper contains 1 times of WORK_OF_ART\n",
        "\n",
        "================================================================================\n",
        "\n",
        "Most similar words to “study” are (descending order): research, analysis, analyses\n"
      ],
      "metadata": {
        "id": "5W3bZWnF6IjQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task4: Summarizing content "
      ],
      "metadata": {
        "id": "C3advl3b4_E9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "summarize_sent1= \"They baked the pizza with pineapples.\"\n",
        "summarize_sent2= \"A respiratory physiotherapist trains a volunteer on how to position himself and behave to  perform the HF-NIV, which requires a monitor, a Phasitron and a nose clip.\"\n",
        "\n",
        "def summary(s):\n",
        "    sent = nlp(s)\n",
        "    root_token = [sentence.root for sentence in sent.sents][0]\n",
        "    for child in root_token.children:\n",
        "      if child.dep_ == 'nsubj':\n",
        "         subj = child\n",
        "      if child.dep_ == 'dobj':\n",
        "         obj = child\n",
        "    return ' '.join([subj.text, root_token.text, obj.text])\n",
        "print(summary(summarize_sent1))\n",
        "print(summary(summarize_sent2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtVLHNXau0X9",
        "outputId": "af5c221f-d722-4277-888f-5ff5811af02e"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "They baked pizza\n",
            "physiotherapist trains volunteer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Task4 results**\n",
        "\n",
        "They baked pizza\n",
        "\n",
        "physiotherapist trains volunteer"
      ],
      "metadata": {
        "id": "gHZTe4Ya6f4R"
      }
    }
  ]
}